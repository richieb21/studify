{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "from youtube_transcript_api.formatters import TextFormatter\n",
    "\n",
    "# Get the video ID of youtube video from url\n",
    "def extract_video_id(url):\n",
    "    if \"youtube.com\" in url:\n",
    "        return url.split(\"v=\")[-1].split(\"&\")[0]\n",
    "    elif \"youtu.be\" in url:\n",
    "        return url.split(\"/\")[-1]\n",
    "    else:\n",
    "        raise ValueError(\"Invalid YouTube URL\")\n",
    "    \n",
    "def load_youtube_transcript(url):\n",
    "    try:\n",
    "        video_id = extract_video_id(url)\n",
    "        transcript = YouTubeTranscriptApi.get_transcript(video_id)\n",
    "        return transcript\n",
    "    except Exception as e:\n",
    "        return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from pinecone import Pinecone, ServerlessSpec, pinecone\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Retrieve API Key from .env file\n",
    "load_dotenv()\n",
    "open_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "\n",
    "# Initialize OpenAI Client\n",
    "client = OpenAI(api_key=open_api_key)\n",
    "\n",
    "# Time to split the transcript into chunks and store in a vector store\n",
    "def collect_transcript(transcript):\n",
    "    text = \"\"\n",
    "    for text_obj in transcript:\n",
    "        text += text_obj[\"text\"] + \" \"\n",
    "\n",
    "    return text\n",
    "\n",
    "def split_text(text, max_length=500):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=max_length,  # The maximum size of each chunk\n",
    "    chunk_overlap=200,  # Overlap between chunks to maintain context\n",
    "    )\n",
    "\n",
    "    return text_splitter.split_text(text)\n",
    "\n",
    "def get_embedding(text, model=\"text-embedding-3-small\"):\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    return client.embeddings.create(input = [text], model=model).data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dimensions = 1536\n",
    "index_name = \"youtube-transcript-index\"\n",
    "\n",
    "# Get Pinecone API Key\n",
    "pinecone_api_key = os.getenv(\"PINECONE_API_KEY\")\n",
    "\n",
    "# Initialize the vector store\n",
    "pc = Pinecone(api_key=pinecone_api_key)\n",
    "\n",
    "if index_name not in pc.list_indexes().names():\n",
    "    pc.create_index(\n",
    "    name=index_name,\n",
    "    dimension=dimensions,\n",
    "    metric=\"cosine\",\n",
    "    spec=ServerlessSpec(\n",
    "        cloud=\"aws\",\n",
    "        region=\"us-east-1\"\n",
    "    ) \n",
    ")\n",
    "\n",
    "index = pc.Index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upsert the embedded vector\n",
    "def upsert_vector(text, embedding):\n",
    "    index.upsert([(text, embedding)])\n",
    "\n",
    "def query_embedding(query_text):\n",
    "    query_embedding = get_embedding(query_text)\n",
    "    result = index.query(vector=[query_embedding], top_k=5)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a Youtube Transcript and store in Pinecone\n",
    "def load_youtube_transcript_and_store(url):\n",
    "    transcript = load_youtube_transcript(url)\n",
    "    chunks = split_text(collect_transcript(transcript))\n",
    "\n",
    "    for chunk in chunks:\n",
    "        embedding = get_embedding(chunk)\n",
    "        upsert_vector(chunk, embedding)\n",
    "\n",
    "# Combine all the documents matched with a query into a single document\n",
    "def combine_documents(documents):\n",
    "    combined_text = \"\"\n",
    "    all_matches = documents[\"matches\"]\n",
    "    for match in all_matches:\n",
    "        combined_text += match[\"id\"] + \"\\n\"\n",
    "    return combined_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response_from_query(query_text):\n",
    "    # Simulate the result of querying embeddings (you would need to implement this part)\n",
    "    result = query_embedding(query_text)\n",
    "    documents = combine_documents(result)\n",
    "\n",
    "    # Construct the prompt manually\n",
    "    system_message = f\"\"\"You are a helpful AI teacher that specializes in whatever subject I give you readings of. \n",
    "    I will provide you with YouTube video transcripts and you will answer questions based on the content. \n",
    "    Here is the relevant content for the question:\\n\\n {documents} \\n\\n Only use information given from the transcript, if you do not have enough context to answer the question, say 'I do not have enough information to answer the question.'\"\"\"\n",
    "    \n",
    "    question_message = f\"Answer the following question: {query_text}\"\n",
    "\n",
    "    # Use the OpenAI API to get the response\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\": question_message}\n",
    "        ],\n",
    "        temperature=0.2\n",
    "    )\n",
    "\n",
    "    # Extract the response content\n",
    "    answer = response.choices[0].message.content\n",
    "    return answer, documents\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To implement your own B-trees, you need to follow these steps based on the provided transcript:\n",
      "\n",
      "1. **Understand the Node Structure**: Each node in a B-tree of degree 4 (as used in the example) can have up to 4 children and 3 keys. Each key in the node has a pointer to the corresponding database record.\n",
      "\n",
      "2. **Insertion Process**: Start by inserting keys into the tree. Initially, there is no node, so the first key you insert becomes the root node.\n",
      "\n",
      "3. **Splitting Nodes**: When a node exceeds its capacity (more than 3 keys in this example), you need to split the node. The middle key moves up to the parent node, and the remaining keys are split into two nodes. This process continues as you insert more keys, causing the tree to grow upwards.\n",
      "\n",
      "4. **Guidelines for Creation**: Unlike a simple multi-way search tree, B-trees have specific rules for creation to ensure efficient searching. These rules help maintain balance in the tree, preventing it from becoming inefficient like a linear search.\n",
      "\n",
      "5. **Multi-Level Indexing**: B-trees are useful for implementing multi-level indexing. As you add keys, the tree automatically creates higher-level indices, making the search process more efficient.\n",
      "\n",
      "By following these steps and guidelines, you can implement your own B-trees.\n"
     ]
    }
   ],
   "source": [
    "video_url = \"https://www.youtube.com/watch?v=aZjYr87r1b8\"\n",
    "# How do I implement my own B-trees?\n",
    "query = input()\n",
    "response, documents = get_response_from_query(query)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study-rag-gpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
